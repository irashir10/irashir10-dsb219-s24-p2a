# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c5FXHYn8mKUEqZfz79BvXkMzEs0Bgrkh
"""

def fasta_reader_dict(filename):
    fasta_dict = {}
    with open(filename, 'r') as file:
        read_id = None
        sequence = []
        for line in file:
            line = line.strip()
            if line.startswith('>'):
                if read_id:
                    fasta_dict[read_id] = ''.join(sequence)
                read_id = line[1:]
                sequence = []
            else:
                sequence.append(line)
        if read_id:
            fasta_dict[read_id] = ''.join(sequence)
    return fasta_dict

def read_fasta_list(file_path):
    reads = []
    with open(file_path, 'r') as file:
        sequence = ''
        for line in file:
            line = line.strip()
            if line.startswith('>'):
                if sequence:
                    reads.append(sequence)
                    sequence = ''
            else:
                sequence += line
        if sequence:
            reads.append(sequence)
    return reads

fasta_filename = 'project2_sample1_spectrum.fasta'
spectrum_reads = fasta_reader_dict(fasta_filename)
print(spectrum_reads)
#for x, y in spectrum_reads.items():
    #print(x, y)

k = 20
t = 3

#seq = 'ATGCATCGGTGAATAAGGTG'
#spec = {'read_0': 'ATGC', 'read_1': 'ATCG', 'read_2': 'GTGA','read_3': 'ATAA', 'read_4': 'GGTG', 'read_5':'ATGC' }


#kmers = ['ATG', 'TGA', 'GAT', 'ATC', 'TCA']
#db_graph = {'ATG': ['TG', 'TC'], 'TGA': ['GA'], 'GAT': ['AT'], 'ATC': ['TC'], 'TCA': ['CA']}

def generate_kmers(sequences, k):
    kmers_list = {}
    for h, seq  in sequences.items():
        for i in range(len(seq) - k + 1):
            kmer = seq[i:i+k]
            if kmer not in kmers_list:  # Ensure each k-mer is added only once
                kmers_list[kmer] = 1
    return kmers_list

kmers = generate_kmers(spectrum_reads, k)
print(kmers)

def de_bruijn_string(kmer_dict):
    from collections import defaultdict
    de_bruijn_graph = defaultdict(list)
    #for kmer, count in kmer_dict.items():
    for kmer in kmer_dict:
        #count = kmer_dict[kmer]
        prefix = kmer[:-1]
        suffix = kmer[1:]
        #for _ in range(count):  # Add suffix multiple times based on frequency
        de_bruijn_graph[prefix].append(suffix)
    return de_bruijn_graph
db_graph = de_bruijn_string(kmers)

print(db_graph)

def ep(db_graph):
    from collections import defaultdict
    in_degree = defaultdict(int)
    out_degree = defaultdict(int)

    for node in db_graph:
        out_degree[node] += len(db_graph[node])
        for neighbor in db_graph[node]:
            in_degree[neighbor] += 1

    start_nodes = None
    end_nodes = None

    for node in set(in_degree.keys()).union(set(out_degree.keys())):
        out_diff = out_degree[node] - in_degree[node]
        if out_diff == 1:
            start_nodes = node
        if out_diff == -1:
            end_nodes = node

    if start_nodes is None:
        start_nodes = next(iter(db_graph))

    path = []
    stack = [start_nodes]

    while stack:
        current_node = stack[-1]
        if db_graph[current_node]:
            next_node = db_graph[current_node].pop(0)
            stack.append(next_node)
        else:
            path.append(stack.pop())


    return path[::-1]

'''

def euler_path(graph):
    # Initialize variables
    path = []
    stack = []
    current_vertex = next(iter(graph))

    # Main algorithm
    while True:
        if graph[current_vertex]:
            stack.append(current_vertex)
            next_vertex = graph[current_vertex].pop(0)
            current_vertex = next_vertex
        else:
            path.append(current_vertex)
            if stack:
                current_vertex = stack.pop()
            else:
                break

    # Return the Eulerian path
    return path[::-1]

# Test the function
eulerian_path = euler_path(db_graph)
print("Eulerian Path:", eulerian_path)


'''
from collections import defaultdict

def calculate_degree(db_graph):
    in_degree = defaultdict(int)
    out_degree = defaultdict(int)

    for node in db_graph:
        out_degree[node] += len(db_graph[node])
        for neighbor in db_graph[node]:
            in_degree[neighbor] += 1

    return in_degree, out_degree

def eulerian_path(db_graph):
    in_degree, out_degree = calculate_degree(db_graph)
    start_nodes, end_nodes = None, None

    for node in set(in_degree.keys()).union(set(out_degree.keys())):
        out_diff = out_degree[node] - in_degree[node]
        if out_diff == 1:
            start_nodes = node
        if out_diff == -1:
            end_nodes = node

    if start_nodes is None:
        start_nodes = next(iter(db_graph))

    path = []
    stack = [start_nodes]

    while stack:
        current_node = stack[-1]
        if db_graph[current_node]:
            next_node = db_graph[current_node].pop(0)
            stack.append(next_node)
        else:
            path.append(stack.pop())

    return path[::-1]


from collections import Counter, deque
def find_start_node(de_bruijn_graph):
    out_degree = Counter()
    in_degree = Counter()

    for node in de_bruijn_graph:
        out_degree[node] += len(de_bruijn_graph[node])
        for adj in de_bruijn_graph[node]:
            in_degree[adj] += 1

    start_node = None
    for node in de_bruijn_graph:
        if out_degree[node] - in_degree[node] == 1:
            start_node = node
            break
        if out_degree[node] > 0 and start_node is None:
            start_node = node

    return start_node
def find_eulerian_path(de_bruijn_graph):
    # Hierholzerâ€™s Algorithm to find Eulerian path
    start_node = find_start_node(de_bruijn_graph)
    stack = [start_node]
    path = []
    current_path = deque()

    while stack:
        u = stack[-1]
        if de_bruijn_graph[u]:
            v = de_bruijn_graph[u].pop()
            stack.append(v)
        else:
            current_path.appendleft(u)
            stack.pop()

    return list(current_path)

UP = find_eulerian_path(db_graph)
print("Eulerian Path:", len(UP))
'''
eulerian_path =  ep(db_graph)
print(eulerian_path)
print(''.join(eulerian_path))

'''

'''
def reconstruct_sequence(eulerian_path):
    reconstructed_seq = eulerian_path[0]

    for i in range(1, len(eulerian_path)):
        seq = eulerian_path[i]
        overlap = find_overlap(reconstructed_seq, seq)
        if overlap > 0:
            reconstructed_seq += seq[overlap:]
        else:
            reconstructed_seq += seq

    return reconstructed_seq

def find_overlap(seq1, seq2):
    max_overlap = min(len(seq1), len(seq2))
    for i in range(max_overlap, 0, -1):
        if seq1.endswith(seq2[:i]):
            return i
    return 0
'''

def full_string(UP):
    genome = UP[0]
    for node in UP[1:]:
        genome += node[-1]
    return genome

final_seq = full_string(UP)
print(final_seq)

# checks
'''
def check_sequence_lengths(eulerian_path):
    for i, seq in enumerate(eulerian_path):
        print(f"Sequence {i + 1} length: {len(seq)}")


def check_last_characters(eulerian_path):
    for i, seq in enumerate(eulerian_path):
        print(f"Last character of Sequence {i + 1}: {seq[-1]}")
x = check_sequence_lengths(eulerian_path)
print(x)

y = check_last_characters(eulerian_path)
print(y)
'''

with open('eulerian_path.txt', 'w') as file:
    for i in eulerian_path:
        file.write(i + '\n')

reconstructed_genome = ''.join(eulerian_path)
with open('reconstructed_genome.txt', 'w') as file:
    #for i in reconstructed_genome:
    file.write(final_seq + '\n')

#compare outputs of ref and reconstructed
ref_genome_sample = read_fasta_list('project2_sample1_reference_genome.fasta')
print(' ',final_seq)
print(ref_genome_sample)

def hamming_distance(seq1, seq2):
    if len(seq1) != len(seq2):
        raise ValueError("not equal length")
        #'nope'
    distance = 0
    for i in range(len(seq1)):
        if seq1[i] != seq2[i]:
            distance += 1
    return distance

def read_mapping_with_substitutions(reads, reference_genome, threshold):
    pos_dict = {}
    for seq_key, seq_value in reads.items():
        read_length = len(seq_value)
        for i in range(len(reference_genome) - read_length + 1):
            if hamming_distance(seq_value, reference_genome[i:i+read_length]) <= threshold:
                if seq_key not in pos_dict:
                    pos_dict[seq_key] = []
                pos_dict[seq_key].append(i)
    return pos_dict

reads_order = read_mapping_with_substitutions(spectrum_reads, final_seq, t)
print(len(reads_order))
# Print items in order
for seq, positions in sorted(reads_order.items(), key=lambda x: min(x[1])):
    print(">"+ seq)

def find_duplicates(reads_list):
    unique_reads_set = set()
    duplicates = []
    for read in reads_list:
        if read in unique_reads_set:
            duplicates.append(read)
        else:
            unique_reads_set.add(read)
    return duplicates

spect = read_fasta_list(fasta_filename)



duplicate_reads = find_duplicates(spect)
if duplicate_reads:
    print("Duplicates found:", duplicate_reads)

print(len(duplicate_reads))
